\documentclass{article}

\title{02258 Parallel Computer Systems Formula Collection}
\author{Jonas Birkedal Dudal Jensen (s203849)}
\date{}

\begin{document}
	\maketitle
	\newpage

	\tableofcontents
	\newpage

	\section{Pipeline Speedup, Throughput, and Results per Cycle (p. 10)}
		Image of pipeline can be seen in figure 1.5 page 9, and figure 1.6 on page 11 shows the throughput for different $m$.

		\subsection{Speedup}
		\begin{equation}
			\frac{T_{seq}}{T_{pipe}}=\frac{mN}{N+m-1}
		\end{equation}
		Where $T_{seq}$ is the sequential execution time, $T_{pipe}$ is the pipelined execution time, $m$ is the depth of the pipeline and $N$ is the number of times it should be executed.

		\subsection{Throughput}
		\begin{equation}
			\frac{N}{T_{pipe}}=\frac{1}{1+\frac{m-1}{N}}
		\end{equation}
		Where $N$ is the number of times it should be executed, $T_{pipe}$ is the pipelined execution time, and $m$ is the depth of the pipeline.

		\subsection{Results per cycle}
		\begin{equation}
			N_c=\frac{(m-1)p}{1-p}
		\end{equation}
		Where $N_c$ is the size of the pipeline resulting in $p$ results per cycle, and $m$ is the depth of the pipeline.
	
	\section{Caches and speed (p. 16)}
		Image of cache performance gain agains cache reuse ratio is seen in figure 1.9 page 16.

		\begin{equation}
			G(\tau, \beta)=\frac{T_m}{T_{av}}=\frac{\tau}{\beta+\tau(1-\beta)}
		\end{equation}
		Where $G$ is the performance gain, $T_m$ is the access time to main memory, $T_{av}$ is the average access time, $\tau$ is the speedup factor of the memory compared to main memory, and $\beta$ is the cache reuse ratio.
	
	\section{Prefetching (p. 21)}
		An example of prefetching is shown in figure 1.13 on page 22.
	
		\begin{equation}
			P=1+\frac{T_l B}{L_c}
		\end{equation}
		Where $P$ is the number of prefetches required to saturate the pipeline, $T_l$ is the latency of a miss, $B$ is the bandwidth, and $L_c$ is the length of a cache line.
	
	\section{Machine and Code Balance (p. 63)}
		\subsection{Machine Balance}
			\begin{equation}
				B_m=\frac{b_{max}}{P_{max}}
			\end{equation}
			Where $B_m$ is the machine balance of the machine, $b_{max}$ is the maximal memory bandwidth (Words$/$sec), and $P_{max}$ is the peak performance (Flops$/$sec).

		\subsection{Code Balance}
			\begin{equation}
				B_c=\frac{d}{o}
			\end{equation}
			Where $B_c$ is the code balance of the program, $d$ is the data trafic per flop (Words$/$sec), and $o$ is the number of floating point ops (Flops$/$sec).
		
		\subsection{Computational Intensity}
			\begin{equation}
				I=\frac{1}{B_c}=\frac{o}{d}
			\end{equation}
			Where $I$ is the computational intensity of the program, $B_c$ is the code balance of the program, $d$ is the data trafic per flop (Words$/$sec), and $o$ is the number of floating point ops (Flops$/$sec).
		
		\subsection{Lightspeed}
			\begin{equation}
				l=\min\left(1,\frac{B_m}{B_c}\right)
			\end{equation}
			Where $l$ is the lightspeed of a loop, or how much the loop is bandwidth limited, $B_m$ is the machine balance of the machine, and $B_c$ is the code balance of the program.
		
		\subsection{Performance from Lightspeed}
			For STREAM specific performances, see (3.5) page 68.
			\begin{equation}
				P=lP_{max}=\min\left(P_{max},\frac{b_{max}}{B_c}\right)
			\end{equation}
			Where $P$ is the performance of running the code on the machine, $l$ is the lightspeed of a loop, $P_{max}$ is the peak performance, $b_{max}$ is the maximal memory bandwidth, and $B_c$ is the code balance of the program.
	
	\section{Communication Bandwidth and Trasfer Time (p. 105)}
		To calculate the increase in effective bandwidth see (4.3) page 108.

		\begin{equation}
			T=T_l+\frac{N}{B}
		\end{equation}
		Where $T$ is the total transfer time, $T_l$ is the latency, $N$ is the nuber of bytes in the message, and $B$ is the bandwidth.

		\begin{equation}
			B_{eff}=\frac{N}{T_l+\frac{N}{B}}
		\end{equation}
		Where $B_{eff}$ is the effective bandwidth of the communication, $T$ is the total transfer time, $T_l$ is the latency, $N$ is the nuber of bytes in the message, and $B$ is the bandwidth.
	
	\section{Strong Scaling and Ahmdal's Law (p. 123)}
		For calculations of the performance of the serial and parallel programs, see (5.5) and (5.6) on page 123.

		\subsection{Strong Scaling Execution Time}
			\begin{equation}
				T=s+\frac{p}{N}
			\end{equation}
			Where $T$ is the time it takes to execute the program, $s$ is the time it takes to execute the serial part of the program, $p$ is the time it takes to execute the parallel part of the program, and $N$ is the number of processes.

		\subsection{Ahmdal's Law}
			\begin{equation}
				S=\frac{P_p}{P_s}=\frac{1}{s+\frac{1-s}{N}}
			\end{equation}
			Where $S$ is the speedup, $P_p$ is the performance of the parallel part, $P_s$ is the performance of the serial part, $s$ is the time it takes to execute the serial part of the program, $p$ is the time it takes to execute the parallelizable part of the program, and $N$ is the number of processes.
	
	\section{Week Scaling (p. 123)}
		\subsection{Week Scaling Execution Time}
			\begin{equation}
				T=s+pN^{\alpha-1}
			\end{equation}
			Where $T$ is the time it takes to execute the program, $s$ is the time it takes to execute the non-parallelizable part of the program, $p$ is the time it takes to execute the parallelizable part of the program, $N$ is the number of processes, and $\alpha$ is a constant defining how much the work amount scales with $N$.

		\subsection{Week Scaling Speedup}
			For another definition of work in week scaling, see (5.17) on page 125.

			\begin{equation}
				S=1+\frac{p}{s}N^{\alpha}
			\end{equation}
			Where $S$ is the speedup, $s$ is the time it takes to execute the non-parallelizable part of the program, $p$ is the time it takes to execute the parallelizable part of the program, $N$ is the number of processes, and $\alpha$ is a constant defining how much the work amount scales with $N$.

		\subsection{Gustafon's Law}
			\begin{equation}
				S=s+(1-s)N
			\end{equation}
			Where $S$ is the speedup when work load increases linearly with $N$, $s$ is the time it takes to execute the non-parallelizable part of the program, and $N$ is the number of processes.
	
	\section{Parallel Efficiency (p. 125)}
		\begin{equation}
			\varepsilon=\frac{S}{N}
		\end{equation}
		Where $\varepsilon$ is the effeciency of the parallelization, meaning how much it is utilized, $S$ is the speedup of the program, and $N$ is the number of processes.

		\subsection{Week Scaling Effeciency}
			For another definition of work in week scaling, see (5.20) on page 126.

			\begin{equation}
				\varepsilon=\frac{sN^{-\alpha}+1(1-s)}{sN^{1-\alpha}+1(1-s)}
			\end{equation}
			Where $\varepsilon$ is the effeciency of the parallelization, $s$ is the time it takes to execute the serial part of the program, $p$ is the time it takes to execute the parallel part of the program, $N$ is the number of processes, and $\alpha$ is a constant defining how much the work amount scales with $N$.
	
	\section{Crossover Point for when to Optimize Serial vs. Parallel}
			\begin{equation}
				N\geq\frac{1}{s}-1
			\end{equation}
			Where $N$ is the number of workers needed before optimizing the serializable part is more efficient than optimizing the parallel part, and $s$ is the executiontime of the serial part of the program.
	
	\section{Refined Performance Models}
			For a list of speedups with non-zero communication overhead, see page 128 and 129.
\end{document}